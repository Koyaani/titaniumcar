{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de la decision forte - Pytorch Edition\n",
    "\n",
    "Après avoir entrainer le modèle, nous testons dans ce notebook ses prédictions avec une vidéo. Il y a en plus la 1ère définition d'une fonction d'inertie pour limiter l'impact des valeurs abérantes.\n",
    "\n",
    "Cf section XXX du rapport\n",
    "\n",
    "### Segmentation du code\n",
    "\n",
    " 1. Définition des constantes\n",
    " 2. Création du pipeline de preprocessing\n",
    " 4. Création du même réseau que lors de l'entrainement\n",
    " 8. Création de la fonction d'inertie\n",
    " 9. Chargment du modèle et application à une vidéo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../../data/prelabeling_videos/\"\n",
    "OUTPUT_PATH = \"../../data/outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du pipeline de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = sample.transpose((2, 0, 1))\n",
    "        image = image[None, :, :, :]\n",
    "        return torch.from_numpy(image)\n",
    "\n",
    "\n",
    "class Normalize():\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        norm_img = np.zeros(image.shape)\n",
    "        norm_img = cv2.normalize(image, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "        norm_img = norm_img.reshape(69, 223, -1)\n",
    "        \n",
    "        return norm_img\n",
    "    \n",
    "class Crop:\n",
    "    def __call__(self, sample):\n",
    "        width = sample.shape[1]\n",
    "        return sample[45:, :width-5]\n",
    "    \n",
    "\n",
    "class Resize():\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.resize(image, (0,0), fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du même réseau que lors de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2,2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(468, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.cnn_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la fonction d'inertie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(target, current, inertia):\n",
    "    offset = log(1.1+abs(target - current)) * (1 - inertia)\n",
    "            \n",
    "    if current > target:\n",
    "        offset *= -1\n",
    "        if current + offset < target:\n",
    "            new = target\n",
    "        else:\n",
    "            new = current + offset\n",
    "    elif current < target:\n",
    "        if current + offset > target:\n",
    "            new = target\n",
    "        else:\n",
    "            new = current + offset\n",
    "    else:\n",
    "        new = target\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargment du modèle et application à une vidéo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(FILE_PATH + 'vid_lataaatqmqcrndd.avi')\n",
    "out = cv2.VideoWriter(\n",
    "    OUTPUT_PATH + 'deep_prediction_pytorch.mp4',\n",
    "    cv2.VideoWriter_fourcc('M','J','P','G'), 20, (456,228)\n",
    ")\n",
    "\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    tsfrm = transforms.Compose([Resize(), Crop(), Normalize(), ToTensor()])\n",
    "    \n",
    "    nn_model = Network().cpu()\n",
    "    nn_model.load_state_dict(torch.load(\"../../data/weights/weights_torch_v3_19.weights\"))\n",
    "    nn_model.eval()\n",
    "    \n",
    "    current_dir = 0\n",
    "    current_speed = 0\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        # Capture frame-by-frame\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            #frame = cv2.flip(frame, 1)\n",
    "            image = tsfrm(frame).float()\n",
    "            prediction = nn_model.forward(image)\n",
    "            \n",
    "            current_dir = compute(prediction[0, 0], current_dir, 0.6)\n",
    "            current_speed = compute(prediction[0, 1], current_speed, 0.8)\n",
    "            \n",
    "            val_dir = current_dir*228\n",
    "            val_dir += 228\n",
    "            val_dir = int(val_dir)\n",
    "            val_speed = current_speed*114\n",
    "            val_speed += 114\n",
    "            val_speed = int(val_speed)\n",
    "            \n",
    "        \n",
    "            cv2.circle(frame, (val_dir, val_speed), 10, (255, 0, 0), -1)\n",
    "            #cv2.circle(frame, (20, val_speed), 10, (0, 255, 0), -1)\n",
    "            out.write(frame)\n",
    "        else: \n",
    "            break\n",
    "\n",
    "out.release()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
